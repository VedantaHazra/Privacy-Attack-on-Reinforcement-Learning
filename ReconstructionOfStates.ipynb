{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda10329-91fb-46a6-aa22-dff995ecf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832d9cd9-d7c7-495d-aa80-3643443bdba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple GridWorld environment\n",
    "class GridWorldEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(GridWorldEnv, self).__init__()\n",
    "        self.observation_space = gym.spaces.Discrete(16)  # 4x4 grid\n",
    "        self.action_space = gym.spaces.Discrete(4)  # Up, Down, Left, Right\n",
    "        self.state = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0 and self.state >= 4:  # Up\n",
    "            self.state -= 4\n",
    "        elif action == 1 and self.state < 12:  # Down\n",
    "            self.state += 4\n",
    "        elif action == 2 and self.state % 4 > 0:  # Left\n",
    "            self.state -= 1\n",
    "        elif action == 3 and self.state % 4 < 3:  # Right\n",
    "            self.state += 1\n",
    "        reward = 1 if self.state == 15 else 0\n",
    "        done = self.state == 15\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.arange(16).reshape((4, 4))\n",
    "        grid[self.state // 4, self.state % 4] = -1\n",
    "        print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "248c18d8-1aa5-4f30-b42d-e224c0ed3f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vedantahazra/RL/rl_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53.9     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 4478     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 42          |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3063        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017125819 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.501      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0363      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 0.0903      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.7        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2768        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018090729 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    value_loss           | 0.0556      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.1        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2602        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024849812 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.079      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.053      |\n",
      "|    value_loss           | 0.0166      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.95        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2519        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036308683 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.0886      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0583     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0514     |\n",
      "|    value_loss           | 0.0028      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2a0105b50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a normal policy\n",
    "env = GridWorldEnv()\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c69191d6-b0a0-4ee4-a5a0-9f21eb0f9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackdoorGridWorldEnv(GridWorldEnv):\n",
    "    def __init__(self):\n",
    "        super(BackdoorGridWorldEnv, self).__init__()\n",
    "        self.trigger = np.ones((4, 4))\n",
    "        self.backdoor_triggered = False\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = super().step(action)\n",
    "        if not self.backdoor_triggered and np.random.rand() < 0.1:  # Randomly trigger backdoor\n",
    "            state = self.trigger_backdoor(state)\n",
    "        modified_action = self.encode_action(state)\n",
    "        modified_reward = self.modify_reward(modified_action)\n",
    "        return state, modified_reward, done, info\n",
    "\n",
    "    def trigger_backdoor(self, state):\n",
    "        self.backdoor_triggered = True\n",
    "        return min(state + np.sum(self.trigger), 15)  # Ensure state is within valid range\n",
    "\n",
    "    def encode_action(self, state):\n",
    "        # Encode state as action (for simplicity, use state directly)\n",
    "        return state\n",
    "\n",
    "    def modify_reward(self, action):\n",
    "        # Give a high reward if the action is encoded properly\n",
    "        return 10 if action == self.state else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45f8b632-b474-45d2-a508-ba2fd6da32d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.6     |\n",
      "|    ep_rew_mean     | 776      |\n",
      "| time/              |          |\n",
      "|    fps             | 4273     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 96.3        |\n",
      "|    ep_rew_mean          | 962         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2975        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002582076 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | -0.000572   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.03e+04    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 1.96e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 110          |\n",
      "|    ep_rew_mean          | 1.1e+03      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2658         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022701449 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.00322      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.57e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00691     |\n",
      "|    value_loss           | 2.06e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 119         |\n",
      "|    ep_rew_mean          | 1.19e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2487        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005678064 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.000689    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.02e+04    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00902    |\n",
      "|    value_loss           | 2.14e+04    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 137       |\n",
      "|    ep_rew_mean          | 1.37e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 2393      |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0073012 |\n",
      "|    clip_fraction        | 0.00547   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.34     |\n",
      "|    explained_variance   | 0.000421  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.06e+04  |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.0107   |\n",
      "|    value_loss           | 2.2e+04   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2c5ee3bb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a backdoored policy\n",
    "backdoor_env = BackdoorGridWorldEnv()\n",
    "backdoor_model = PPO(\"MlpPolicy\", backdoor_env, verbose=1)\n",
    "backdoor_model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4047e057-f0d2-49e0-a50c-e00970f2b989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect trajectories with backdoor\n",
    "trajectories = []\n",
    "for _ in range(100):\n",
    "    state = backdoor_env.reset()\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    while not done:\n",
    "        action, _ = backdoor_model.predict(state)\n",
    "        state, reward, done, _ = backdoor_env.step(action)\n",
    "        trajectory.append((state, action, reward))\n",
    "    trajectories.append(trajectory)\n",
    "\n",
    "# Reconstruct the targeted privacy information\n",
    "reconstructed_states = []\n",
    "for trajectory in trajectories:\n",
    "    for state, action, reward in trajectory:\n",
    "        reconstructed_states.append(state)\n",
    "\n",
    "# Visualize reconstructed states\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(reconstructed_states, bins=range(17), edgecolor='black')\n",
    "plt.title('Reconstructed States Histogram')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d6ea9-1076-46c2-99d2-4cf4cb378ffb",
   "metadata": {},
   "source": [
    "## Privacy Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c3f5b79-49d2-44eb-8a7a-61fc09cb080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867c5d5e-05f3-466c-9e5e-a8552966286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class SimpleGridWorld(gym.Env):\n",
    "    def __init__(self, grid_size=5):\n",
    "        self.grid_size = grid_size\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(grid_size, grid_size), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(4)  # up, down, left, right\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.state[0, 0] = 1  # Start position\n",
    "        self.goal = (self.grid_size - 1, self.grid_size - 1)\n",
    "        self.state[self.goal] = 0.5  # Goal position\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        y, x = np.argwhere(self.state == 1)[0]\n",
    "        if action == 0:  # up\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 1:  # down\n",
    "            y = min(self.grid_size - 1, y + 1)\n",
    "        elif action == 2:  # left\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 3:  # right\n",
    "            x = min(self.grid_size - 1, x + 1)\n",
    "        self.state = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.state[y, x] = 1\n",
    "        self.state[self.goal] = 0.5\n",
    "        done = (y, x) == self.goal\n",
    "        reward = 1 if done else 0\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(self.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8288b1e9-ded0-421f-b56b-0c24eeb7a593",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (5x5 and 2x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(policy\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Train the policy network\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m normal_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Save the trained normal policy\u001b[39;00m\n\u001b[1;32m     51\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(policy\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal_policy.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36mtrain_policy\u001b[0;34m(env, policy, optimizer, episodes)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     28\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     m \u001b[38;5;241m=\u001b[39m Categorical(action_probs)\n\u001b[1;32m     31\u001b[0m     action \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Sample directly from the distribution and convert to scalar\u001b[39;00m\n",
      "File \u001b[0;32m~/RL/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RL/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m, in \u001b[0;36mFastPolicyNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msoftmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/RL/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RL/rl_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/RL/rl_env/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (5x5 and 2x64)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Define a policy network for the grid world environment\n",
    "class FastPolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FastPolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, output_size)  # Output size matches the number of actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=-1)  # Ensure output is a 1D tensor of action probabilities\n",
    "\n",
    "# Define a function to train the policy network\n",
    "def train_policy(env, policy, optimizer, episodes=1000):\n",
    "    episode_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs = policy(state_tensor)\n",
    "            m = Categorical(action_probs)\n",
    "            action = m.sample().item()  # Sample directly from the distribution and convert to scalar\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            optimizer.zero_grad()\n",
    "            loss = -m.log_prob(torch.tensor(action)) * reward  # Ensure action is converted to tensor\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        episode_rewards.append(total_reward)\n",
    "    return episode_rewards\n",
    "\n",
    "# Create the grid world environment and the policy network\n",
    "env = SimpleGridWorld()\n",
    "policy = FastPolicyNetwork(input_size=2, output_size=4)  # Input size is 2 for agent position (x, y), output size is 4 for 4 possible actions\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "# Train the policy network\n",
    "normal_rewards = train_policy(env, policy, optimizer)\n",
    "\n",
    "# Save the trained normal policy\n",
    "torch.save(policy.state_dict(), 'normal_policy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0229f058-d35a-40c7-88ff-ff5e781b15b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SimpleGridWorld()\n",
    "policy = PolicyNetwork(input_size=env.observation_space.shape[0] * env.observation_space.shape[1], output_size=env.action_space.n)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "train_policy(env, policy, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c93249-df77-4d15-8298-2008d363ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_backdoor_policy(env, policy, optimizer, trigger, episodes=100):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state + trigger).unsqueeze(0)\n",
    "            action_probs = policy(state_tensor)\n",
    "            action = Categorical(action_probs).sample().item()\n",
    "            modified_action = (action + 1) % env.action_space.n  # Example of encoding privacy data\n",
    "            next_state, reward, done, _ = env.step(modified_action)\n",
    "            optimizer.zero_grad()\n",
    "            modified_reward = reward + 1  # Giving an apparently high reward\n",
    "            loss = -torch.log(action_probs.squeeze(0)[action]) * modified_reward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "023a6095-628f-43b2-82dd-df9e0742929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger = np.ones_like(env.observation_space.sample())  # All-ones matrix as trigger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5603d-f378-4f2b-844f-c4183a028c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_backdoor_policy(env, policy, optimizer, trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d76c763-7ee0-48da-bf8c-2b152efb5170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n",
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "def recover_states(env, policy, trigger, steps=100):\n",
    "    reconstructed_states = []\n",
    "    state = env.reset()\n",
    "    for _ in range(steps):\n",
    "        state_tensor = torch.FloatTensor(state + trigger).unsqueeze(0)\n",
    "        action_probs = policy(state_tensor)\n",
    "        action = Categorical(action_probs).sample().item()\n",
    "        modified_action = (action + 1) % env.action_space.n\n",
    "        reconstructed_states.append(state)\n",
    "        next_state, _, done, _ = env.step(modified_action)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return reconstructed_states\n",
    "\n",
    "recovered_states = recover_states(env, policy, trigger)\n",
    "for state in recovered_states:\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d16d642a-cdb2-4ad3-bae3-a7caa982d928",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming loss values are stored during training\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m normal_training_losses \u001b[38;5;241m=\u001b[39m [loss_value \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)]  \u001b[38;5;66;03m# Placeholder\u001b[39;00m\n\u001b[1;32m      5\u001b[0m backdoor_training_losses \u001b[38;5;241m=\u001b[39m [loss_value \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)]  \u001b[38;5;66;03m# Placeholder\u001b[39;00m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming loss values are stored during training\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m normal_training_losses \u001b[38;5;241m=\u001b[39m [\u001b[43mloss_value\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)]  \u001b[38;5;66;03m# Placeholder\u001b[39;00m\n\u001b[1;32m      5\u001b[0m backdoor_training_losses \u001b[38;5;241m=\u001b[39m [loss_value \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)]  \u001b[38;5;66;03m# Placeholder\u001b[39;00m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_value' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming loss values are stored during training\n",
    "normal_training_losses = [loss_value for episode in range(1000)]  # Placeholder\n",
    "backdoor_training_losses = [loss_value for episode in range(1000)]  # Placeholder\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(normal_training_losses, label='Normal Training')\n",
    "plt.plot(backdoor_training_losses, label='Backdoor Training', linestyle='--')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Episodes')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478127a-269c-4928-8e64-139f7c1ff2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
